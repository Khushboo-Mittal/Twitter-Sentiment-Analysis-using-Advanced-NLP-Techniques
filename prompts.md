# DIY NLP - Text Preprocessing for Twitter Sentiment Anaylsis

## **Business Problem Statement**
In todayâ€™s social media-driven world, analyzing sentiment on platforms like Twitter is crucial for businesses aiming to understand public opinion and trends. This project seeks to develop an NLP pipeline capable of processing Twitter sentiment data to gain insights into how entities (such as brands, products, or individuals) are perceived. The primary objective is to create a DIY solution that includes all foundational NLP processing techniques (stemming, lemmatization, count vectorization, TF-IDF) and advanced embeddings (Word2Vec, GloVe). Additionally, by implementing a custom Encoder-Decoder model in PyTorch, the project will explore sentiment prediction using an unsupervised deep learning approach.

## Introduction to NLP Text Preprocessing  

1. What is the role of text preprocessing in preparing data for NLP models?  
2. How does preprocessing impact the quality and performance of NLP tasks?  
3. Why is it important to remove noise and inconsistencies in text data?   
4. What are some challenges in preprocessing informal text, such as social media content?  
5. How does text preprocessing differ for structured and unstructured data?  
6. Provide an overview of a typical NLP preprocessing pipeline.  

## Basic Text Preprocessing Steps  

1. What is tokenization, and how do different tokenization techniques affect text representation?  
2. Why and how do you remove stop words from text data?  
3. Explain the difference between stemming and lemmatization and when to use each.  
4. How do you handle case sensitivity in text preprocessing?  
5. What are the methods to remove punctuation, special characters, and numbers from text?  
6. How do you preprocess contractions, abbreviations, and slang in text?  
7. Explain the importance of handling white spaces and formatting issues in text.  


## Count Vectorizer  

1. What is a Count Vectorizer, and how does it convert text into numerical data?  
2. How does the size of the vocabulary affect the vectorized representation in Count Vectorizer?  
3. Explain the impact of text preprocessing on Count Vectorizer outputs.  
4. What are the trade-offs of using unigrams versus n-grams in Count Vectorizer?  
5. How do you handle high-dimensional sparse matrices generated by Count Vectorizer?  
6. What are some use cases where Count Vectorizer is preferable over dense embeddings?  
7. How do you visualize the sparse matrix output of Count Vectorizer using SciPy?  

## TF-IDF Vectorizer  

1. What is the concept of Term Frequency-Inverse Document Frequency (TF-IDF), and how does it differ from raw counts?  
2. How does TF-IDF prioritize important words in a document?  
3. What is the role of normalization in TF-IDF, and how does it affect results?  
4. How do you handle sparse matrices generated by TF-IDF using SciPy?  
5. How does preprocessing like stop-word removal improve TF-IDF results?    
6. What are the advantages of TF-IDF over Count Vectorizer for text representation?  
 

## Word2Vec  

1. What is Word2Vec, and how does it create dense word embeddings?  
2. Explain the Skip-Gram model in Word2Vec and its application.    
3. How do you train Word2Vec embeddings on a custom text corpus?  
4. How do pre-trained Word2Vec embeddings improve NLP tasks?  
5. Explain how Word2Vec captures semantic relationships between words.  
6. What are some challenges in training Word2Vec for large datasets?  
 

## GloVe Vectorizer  

1. What is GloVe, and how does it use co-occurrence matrices to generate embeddings?  
2. Explain how GloVe embeddings capture both semantic and syntactic word relationships.  
3. How do you use pre-trained GloVe embeddings in NLP projects?  
4. What are the advantages of GloVe embeddings over Word2Vec in certain applications?  
5. How do you adapt GloVe embeddings for domain-specific text data?  
6. Explain the process of loading and integrating GloVe embeddings into an NLP pipeline.  


## Comparative and Combined Use of Vectorization Techniques  

1. How do Count Vectorizer, TF-IDF, Word2Vec, and GloVe differ in their approaches to text representation?  
2. What are the strengths and weaknesses of each technique for different NLP tasks?  
3. How do you combine multiple vectorization techniques in an ensemble approach?  
4. Explain the trade-offs between sparse and dense vector representations in NLP.  
5. How does preprocessing influence the effectiveness of different vectorization methods?  
6. What are some hybrid techniques that leverage both traditional and embedding-based vectorization?  
7. How do you evaluate the impact of vectorization techniques on model performance?  
8. Compare the scalability of Count Vectorizer, TF-IDF, Word2Vec, and GloVe for large datasets.  
9. Provide a case study where multiple vectorization methods were applied and analyzed.  
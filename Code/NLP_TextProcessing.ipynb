{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META DATA - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Developer details: \n",
    "        # Name:  Harshita Jangde, Prachi Tavse, Tanisha Priya, Khushboo Mittal\n",
    "        # Role: Architect\n",
    "    # Version:\n",
    "        # Version: V 1.0 (24 October 2024)\n",
    "            # Developers: Harshita Jangde, Prachi Tavse, Tanisha Priya, Khushboo Mittal\n",
    "     \n",
    "     # Description: This code snippet implements various Natural Language Processing (NLP) text processing \n",
    "     # techniques, including lemmatization, stemming, count vectorization, TF-IDF, and word embeddings \n",
    "     # (Word2Vec, GloVe). The project preprocesses raw text data for analysis and modeling, enabling \n",
    "     # enhanced insights and efficient machine learning applications. Functions include transformations \n",
    "     # to reduce word forms, vectorize text, and generate word embeddings for downstream NLP tasks.\n",
    "\n",
    "# CODE - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Dependency: \n",
    "        # Environment:     \n",
    "            # Python: 3.10.8\n",
    "            # NLTK: 3.9.1\n",
    "            # scikit-learn: 1.4.2\n",
    "            # gensim: 4.3.1\n",
    "\n",
    "        \n",
    "# Importing necessary libraries for data manipulation, text processing, and NLP\n",
    "import pandas as pd   # For data manipulation\n",
    "import numpy as np    # For numerical operations\n",
    "import nltk           # For natural language processing\n",
    "from nltk.corpus import stopwords           # For removing stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer   # For stemming and lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # For text vectorization\n",
    "from gensim.models import Word2Vec          # For word embeddings\n",
    "from transformers import AutoTokenizer, AutoModel   # For pre-trained transformer models\n",
    "\n",
    "# Load Twitter data from CSV file\n",
    "twitter_data = pd.read_csv('../Data/twitter_data.csv')\n",
    "\n",
    "# Remove rows with missing values in 'tweet_content' column\n",
    "twitter_data.dropna(subset=['tweet_content'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im get borderland murder ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>come border kill all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im get borderland kill all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im come borderland murder all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im get borderland 2 murder all,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       entity sentiment  \\\n",
       "0     2401  Borderlands  Positive   \n",
       "1     2401  Borderlands  Positive   \n",
       "2     2401  Borderlands  Positive   \n",
       "3     2401  Borderlands  Positive   \n",
       "4     2401  Borderlands  Positive   \n",
       "\n",
       "                                       tweet_content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  I am coming to the borders and I will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                 processed_content  \n",
       "0       im get borderland murder ,  \n",
       "1            come border kill all,  \n",
       "2      im get borderland kill all,  \n",
       "3   im come borderland murder all,  \n",
       "4  im get borderland 2 murder all,  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources: stopwords and WordNet data for lemmatization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a set of stopwords in English for filtering out common words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer and lemmatizer for text normalization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase and remove stopwords\n",
    "    tokens = [word for word in text.lower().split() if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming to reduce words to their root form\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Apply lemmatization to convert words to their base form\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    \n",
    "    # Return the processed tokens as a single string\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply the preprocessing function to the 'tweet_content' column\n",
    "twitter_data['processed_content'] = twitter_data['tweet_content'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify changes\n",
    "twitter_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011</th>\n",
       "      <th>00014</th>\n",
       "      <th>00015</th>\n",
       "      <th>00016</th>\n",
       "      <th>00054</th>\n",
       "      <th>00105</th>\n",
       "      <th>00107</th>\n",
       "      <th>00303</th>\n",
       "      <th>...</th>\n",
       "      <th>ÿ≠ÿ®Ÿäÿ™</th>\n",
       "      <th>ÿÆŸÑÿßÿµ</th>\n",
       "      <th>ÿπÿ®ÿ±</th>\n",
       "      <th>ŸÅŸäÿØŸäŸà</th>\n",
       "      <th>Ÿ•œÖ</th>\n",
       "      <th>‡§ò‡§∞‡§ö</th>\n",
       "      <th>‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á‡∏â</th>\n",
       "      <th>‡∏ô‡∏à‡∏≤‡∏Å</th>\n",
       "      <th>‚Ñê‚ÑìŸ•</th>\n",
       "      <th>ùêçùêÑùêñùêíùêîùêèùêÉùêÄùêìùêÑùêí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 29826 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00011  00014  00015  00016  00054  00105  00107  00303  ...  ÿ≠ÿ®Ÿäÿ™  \\\n",
       "0   0    0      0      0      0      0      0      0      0      0  ...     0   \n",
       "1   0    0      0      0      0      0      0      0      0      0  ...     0   \n",
       "2   0    0      0      0      0      0      0      0      0      0  ...     0   \n",
       "3   0    0      0      0      0      0      0      0      0      0  ...     0   \n",
       "4   0    0      0      0      0      0      0      0      0      0  ...     0   \n",
       "\n",
       "   ÿÆŸÑÿßÿµ  ÿπÿ®ÿ±  ŸÅŸäÿØŸäŸà  Ÿ•œÖ  ‡§ò‡§∞‡§ö  ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á‡∏â  ‡∏ô‡∏à‡∏≤‡∏Å  ‚Ñê‚ÑìŸ•  ùêçùêÑùêñùêíùêîùêèùêÉùêÄùêìùêÑùêí  \n",
       "0     0    0      0   0    0                0     0    0            0  \n",
       "1     0    0      0   0    0                0     0    0            0  \n",
       "2     0    0      0   0    0                0     0    0            0  \n",
       "3     0    0      0   0    0                0     0    0            0  \n",
       "4     0    0      0   0    0                0     0    0            0  \n",
       "\n",
       "[5 rows x 29826 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a CountVectorizer to convert text to a matrix of token counts\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the processed text and transform it into a count matrix\n",
    "count_matrix = count_vectorizer.fit_transform(twitter_data['processed_content'])\n",
    "\n",
    "# Import csr_matrix from scipy for sparse matrix representation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert the count matrix to a sparse DataFrame to save memory\n",
    "# Use feature names as column headers in the DataFrame\n",
    "count_sparse_df = pd.DataFrame.sparse.from_spmatrix(count_matrix, columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the sparse DataFrame\n",
    "count_sparse_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011</th>\n",
       "      <th>00014</th>\n",
       "      <th>00015</th>\n",
       "      <th>00016</th>\n",
       "      <th>00054</th>\n",
       "      <th>00105</th>\n",
       "      <th>00107</th>\n",
       "      <th>00303</th>\n",
       "      <th>...</th>\n",
       "      <th>ÿ≠ÿ®Ÿäÿ™</th>\n",
       "      <th>ÿÆŸÑÿßÿµ</th>\n",
       "      <th>ÿπÿ®ÿ±</th>\n",
       "      <th>ŸÅŸäÿØŸäŸà</th>\n",
       "      <th>Ÿ•œÖ</th>\n",
       "      <th>‡§ò‡§∞‡§ö</th>\n",
       "      <th>‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á‡∏â</th>\n",
       "      <th>‡∏ô‡∏à‡∏≤‡∏Å</th>\n",
       "      <th>‚Ñê‚ÑìŸ•</th>\n",
       "      <th>ùêçùêÑùêñùêíùêîùêèùêÉùêÄùêìùêÑùêí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 29826 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00011  00014  00015  00016  00054  00105  00107  00303  ...  \\\n",
       "0  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   ÿ≠ÿ®Ÿäÿ™  ÿÆŸÑÿßÿµ  ÿπÿ®ÿ±  ŸÅŸäÿØŸäŸà   Ÿ•œÖ  ‡§ò‡§∞‡§ö  ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á‡∏â  ‡∏ô‡∏à‡∏≤‡∏Å  ‚Ñê‚ÑìŸ•  ùêçùêÑùêñùêíùêîùêèùêÉùêÄùêìùêÑùêí  \n",
       "0   0.0   0.0  0.0    0.0  0.0  0.0              0.0   0.0  0.0          0.0  \n",
       "1   0.0   0.0  0.0    0.0  0.0  0.0              0.0   0.0  0.0          0.0  \n",
       "2   0.0   0.0  0.0    0.0  0.0  0.0              0.0   0.0  0.0          0.0  \n",
       "3   0.0   0.0  0.0    0.0  0.0  0.0              0.0   0.0  0.0          0.0  \n",
       "4   0.0   0.0  0.0    0.0  0.0  0.0              0.0   0.0  0.0          0.0  \n",
       "\n",
       "[5 rows x 29826 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a TF-IDF Vectorizer to convert text to a matrix of TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the processed text and transform it into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(twitter_data['processed_content'])\n",
    "\n",
    "# Import csr_matrix from scipy for efficient sparse matrix handling\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert the TF-IDF matrix to a sparse DataFrame using CSR format\n",
    "# Use feature names as column headers in the DataFrame\n",
    "tfidf_sparse_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the sparse DataFrame\n",
    "tfidf_sparse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>word2vec_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im get borderland murder ,</td>\n",
       "      <td>[-1.0566607, 0.5728749, -0.63472146, -0.700388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>come border kill all,</td>\n",
       "      <td>[-0.34483135, 0.3098346, -0.2070818, -0.452307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im get borderland kill all,</td>\n",
       "      <td>[-1.0111482, 0.41281447, -0.48897535, -0.79711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im come borderland murder all,</td>\n",
       "      <td>[-0.76692855, 0.5205113, -0.53366506, -0.65859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im get borderland 2 murder all,</td>\n",
       "      <td>[-1.1728438, 0.61338407, -0.59754443, -0.61138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       entity sentiment  \\\n",
       "0     2401  Borderlands  Positive   \n",
       "1     2401  Borderlands  Positive   \n",
       "2     2401  Borderlands  Positive   \n",
       "3     2401  Borderlands  Positive   \n",
       "4     2401  Borderlands  Positive   \n",
       "\n",
       "                                       tweet_content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  I am coming to the borders and I will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                 processed_content  \\\n",
       "0       im get borderland murder ,   \n",
       "1            come border kill all,   \n",
       "2      im get borderland kill all,   \n",
       "3   im come borderland murder all,   \n",
       "4  im get borderland 2 murder all,   \n",
       "\n",
       "                                     word2vec_vector  \n",
       "0  [-1.0566607, 0.5728749, -0.63472146, -0.700388...  \n",
       "1  [-0.34483135, 0.3098346, -0.2070818, -0.452307...  \n",
       "2  [-1.0111482, 0.41281447, -0.48897535, -0.79711...  \n",
       "3  [-0.76692855, 0.5205113, -0.53366506, -0.65859...  \n",
       "4  [-1.1728438, 0.61338407, -0.59754443, -0.61138...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data for Word2Vec by splitting each tweet into a list of words (tokenized sentences)\n",
    "sentences = [tweet.split() for tweet in twitter_data['processed_content']]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized sentences\n",
    "# - vector_size: size of the word vectors\n",
    "# - window: maximum distance between the current and predicted word\n",
    "# - min_count: minimum word frequency to include in the vocabulary\n",
    "# - workers: number of CPU cores to use\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define a function to get the sentence vector by averaging word embeddings\n",
    "def get_sentence_vector(sentence, model):\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Retrieve vectors for each word if it exists in the model's vocabulary\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    # Return the average vector for the sentence; if no words have vectors, return a zero vector\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the function to each processed tweet to get the sentence vector representation\n",
    "twitter_data['word2vec_vector'] = twitter_data['processed_content'].apply(lambda x: get_sentence_vector(x, word2vec_model))\n",
    "twitter_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>word2vec_vector</th>\n",
       "      <th>glove_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im get borderland murder ,</td>\n",
       "      <td>[-1.0566607, 0.5728749, -0.63472146, -0.700388...</td>\n",
       "      <td>[0.22772, 0.0013851999999999949, 0.3162134, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>come border kill all,</td>\n",
       "      <td>[-0.34483135, 0.3098346, -0.2070818, -0.452307...</td>\n",
       "      <td>[-0.28461833333333336, -0.16813666666666668, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im get borderland kill all,</td>\n",
       "      <td>[-1.0111482, 0.41281447, -0.48897535, -0.79711...</td>\n",
       "      <td>[0.089055, 0.04352899999999999, 0.37232425, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im come borderland murder all,</td>\n",
       "      <td>[-0.76692855, 0.5205113, -0.53366506, -0.65859...</td>\n",
       "      <td>[0.25339875, 0.014191499999999996, 0.24798925,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im get borderland 2 murder all,</td>\n",
       "      <td>[-1.1728438, 0.61338407, -0.59754443, -0.61138...</td>\n",
       "      <td>[0.171896, 0.10174520000000001, 0.226181400000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       entity sentiment  \\\n",
       "0     2401  Borderlands  Positive   \n",
       "1     2401  Borderlands  Positive   \n",
       "2     2401  Borderlands  Positive   \n",
       "3     2401  Borderlands  Positive   \n",
       "4     2401  Borderlands  Positive   \n",
       "\n",
       "                                       tweet_content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  I am coming to the borders and I will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                 processed_content  \\\n",
       "0       im get borderland murder ,   \n",
       "1            come border kill all,   \n",
       "2      im get borderland kill all,   \n",
       "3   im come borderland murder all,   \n",
       "4  im get borderland 2 murder all,   \n",
       "\n",
       "                                     word2vec_vector  \\\n",
       "0  [-1.0566607, 0.5728749, -0.63472146, -0.700388...   \n",
       "1  [-0.34483135, 0.3098346, -0.2070818, -0.452307...   \n",
       "2  [-1.0111482, 0.41281447, -0.48897535, -0.79711...   \n",
       "3  [-0.76692855, 0.5205113, -0.53366506, -0.65859...   \n",
       "4  [-1.1728438, 0.61338407, -0.59754443, -0.61138...   \n",
       "\n",
       "                                        glove_vector  \n",
       "0  [0.22772, 0.0013851999999999949, 0.3162134, -0...  \n",
       "1  [-0.28461833333333336, -0.16813666666666668, 0...  \n",
       "2  [0.089055, 0.04352899999999999, 0.37232425, -0...  \n",
       "3  [0.25339875, 0.014191499999999996, 0.24798925,...  \n",
       "4  [0.171896, 0.10174520000000001, 0.226181400000...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to load pre-trained GloVe embeddings from a file\n",
    "def load_glove_model(glove_file):\n",
    "    glove_model = {}\n",
    "    # Open GloVe file and read line by line\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Split each line into the word and its vector components\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]  # The word itself\n",
    "            # Convert the remaining elements to a numpy array as the word vector\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding  # Add word and vector to the GloVe model dictionary\n",
    "    return glove_model\n",
    "\n",
    "# Specify the path to your downloaded GloVe embeddings file (100-dimensional vectors)\n",
    "glove_file_path = 'glove.6B\\glove.6B.100d.txt'\n",
    "glove_model = load_glove_model(glove_file_path) \n",
    "\n",
    "# Define a function to compute the GloVe vector for an entire sentence\n",
    "def get_glove_vector(sentence, model):\n",
    "    words = sentence.split()  # Split sentence into words\n",
    "    # Retrieve vectors for each word if it exists in the GloVe model\n",
    "    word_vectors = [model[word] for word in words if word in model] #For each word, if it‚Äôs in glove_model, its embedding is retrieved.\n",
    "    # Return the average vector for the sentence; if no words have vectors, return a zero vector\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(100)  # For 100-dimensional embeddings\n",
    "\n",
    "# Apply the function to each processed tweet to get the sentence vector representation\n",
    "twitter_data['glove_vector'] = twitter_data['processed_content'].apply(lambda x: get_glove_vector(x, glove_model))\n",
    "\n",
    "# Display the first few rows of the dataframe with GloVe vectors included\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training, Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7827333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.83      0.68      0.75      2666\n",
      "    Negative       0.78      0.84      0.81      4464\n",
      "     Neutral       0.81      0.74      0.77      3706\n",
      "    Positive       0.74      0.83      0.78      4164\n",
      "\n",
      "    accuracy                           0.78     15000\n",
      "   macro avg       0.79      0.77      0.78     15000\n",
      "weighted avg       0.79      0.78      0.78     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model training, evaluation, and data splitting\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and test sets\n",
    "from sklearn.linear_model import LogisticRegression   # For logistic regression model\n",
    "from sklearn.metrics import classification_report, accuracy_score  # For model evaluation metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text to TF-IDF features\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "# Converts processed content into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(twitter_data['processed_content'])  # Feature matrix\n",
    "y = twitter_data['sentiment']  # Target variable, assuming 'sentiment' is present in the data\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy and detailed classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Outputs the overall accuracy\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))  # Detailed report on precision, recall, f1-score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
